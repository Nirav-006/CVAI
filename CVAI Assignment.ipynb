{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef9333b-d6ca-41cf-a080-4acbb2529674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NiravG\\yolov7\\models\\experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(w, map_location=map_location)  # load\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NiravG\\anaconda3\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms as transforms\n",
    "import multiprocessing\n",
    "import sys\n",
    "\n",
    "# Add YOLOv7 to the Python path\n",
    "sys.path.append(r\"C:\\Users\\NiravG\\yolov7\")  # Replace with your yolov7 repo path\n",
    "\n",
    "# Import YOLOv7 specific loading function\n",
    "from models.experimental import attempt_load\n",
    "\n",
    "# Preprocess function for YOLOv7\n",
    "def preprocess_image(image, img_size=640):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize as per ImageNet\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "# Convert boxes to YOLO format (class, x_center, y_center, width, height)\n",
    "def convert_boxes_to_yolo_format(boxes, labels, img_width, img_height):\n",
    "    yolo_boxes = []\n",
    "    for box, label in zip(boxes, labels):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        x_center = ((xmin + xmax) / 2.0) / img_width\n",
    "        y_center = ((ymin + ymax) / 2.0) / img_height\n",
    "        box_width = (xmax - xmin) / img_width\n",
    "        box_height = (ymax - ymin) / img_height\n",
    "        yolo_boxes.append([label, x_center, y_center, box_width, box_height])\n",
    "    return yolo_boxes\n",
    "\n",
    "# PascalVOCDataset class for YOLOv7\n",
    "class PascalVOCDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations_dir, img_size=640):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_filenames = os.listdir(image_dir)\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image\n",
    "        image_filename = self.image_filenames[index]\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        img_width, img_height = image.size\n",
    "\n",
    "        # Preprocess image for YOLOv7\n",
    "        image = preprocess_image(image, self.img_size)\n",
    "\n",
    "        # Load corresponding annotation (XML file)\n",
    "        annotation_filename = os.path.splitext(image_filename)[0] + '.xml'\n",
    "        annotation_path = os.path.join(self.annotations_dir, annotation_filename)\n",
    "        boxes, labels = self._parse_annotation(annotation_path)\n",
    "\n",
    "        # Convert boxes to YOLO format\n",
    "        yolo_boxes = convert_boxes_to_yolo_format(boxes, labels, img_width, img_height)\n",
    "\n",
    "        # Convert to tensor\n",
    "        yolo_boxes = torch.tensor(yolo_boxes, dtype=torch.float32)\n",
    "\n",
    "        return image, yolo_boxes\n",
    "\n",
    "    def _parse_annotation(self, annotation_path):\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            label = obj.find('name').text\n",
    "            label_idx = VOC_CLASSES.index(label)\n",
    "\n",
    "            bbox = obj.find('bndbox')\n",
    "            xmin = float(bbox.find('xmin').text)\n",
    "            ymin = float(bbox.find('ymin').text)\n",
    "            xmax = float(bbox.find('xmax').text)\n",
    "            ymax = float(bbox.find('ymax').text)\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(label_idx)\n",
    "\n",
    "        return boxes, labels\n",
    "\n",
    "# VOC classes\n",
    "VOC_CLASSES = [\n",
    "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \n",
    "    \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \n",
    "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "\n",
    "# Load YOLOv7 model from the local .pt file\n",
    "model_path = r'C:\\Users\\NiravG\\yolov7.pt'\n",
    "device = torch.device('cpu')  # Use 'cuda' if GPU is available\n",
    "model = attempt_load(model_path, map_location=device)\n",
    "\n",
    "# Check model parameters and filter leaf tensors\n",
    "leaf_parameters = [p for p in model.parameters() if p.is_leaf and p.requires_grad]\n",
    "\n",
    "# Warning if no leaf parameters are found\n",
    "if len(leaf_parameters) == 0:\n",
    "    print(\"Warning: No leaf parameters found that require gradients.\")\n",
    "\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "# Paths to image and annotations directories\n",
    "train_image_dir = r'C:\\Users\\NiravG\\Downloads\\archive (8)\\VOC2012\\JPEGImages'\n",
    "train_annotations_dir = r'C:\\Users\\NiravG\\Downloads\\archive (8)\\VOC2012\\Annotations'\n",
    "test_image_dir = r'C:\\Users\\NiravG\\Downloads\\archive (8)\\VOC2012\\JPEGImages'\n",
    "test_annotations_dir = r'C:\\Users\\NiravG\\Downloads\\archive (8)\\VOC2012\\Annotations'\n",
    "\n",
    "# Load the dataset and prepare the data loader\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "train_data = PascalVOCDataset(train_image_dir, train_annotations_dir, img_size=640)\n",
    "test_data = PascalVOCDataset(test_image_dir, test_annotations_dir, img_size=640)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Set up optimizer (SGD for YOLOv7 training)\n",
    "optimizer = torch.optim.Adam(leaf_parameters, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop for YOLOv7\n",
    "for epoch in range(10):  # 10 epochs as an example\n",
    "    running_loss = 0.0\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Ensure each target is converted to the appropriate device and format\n",
    "        targets = [t.to(device) for t in targets]\n",
    "\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass and compute the loss\n",
    "        loss, loss_items = model(images, targets)\n",
    "        \n",
    "        # Ensure the loss is scalar before backward pass\n",
    "        loss = loss.sum() if isinstance(loss, torch.Tensor) and loss.ndim > 0 else loss\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Batch {i}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1} finished. Average Loss: {running_loss / len(train_loader)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
